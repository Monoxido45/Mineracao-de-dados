---
title: "Lista 2 - Mineração de dados"
author: "Luben, Luiz Piccin, Vinicius Hideki"
date: "10/4/2021"
header-includes:
  - \usepackage{amsmath}
  - \usepackage{xcolor}
output: 
  pdf_document:
    fig_caption: yes
    df_print: kable
latex_engine: texlive
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, out.width = '85%', fig.align = "center")
```
```{r message = FALSE, include = FALSE}
# importando os pacotes de interesse
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(GGally)
library(ggcorrplot)
library(reshape2)
library(tidyverse)
library(magrittr)


# pacotes do lasso, ridge, elastic net, validação e tidyverse
library(glmnet)
library(caret)
library(tidyverse)
library(magrittr)

# usando o tidymodels
library(tidymodels)
```
# Exercício 1: \
Podemos implementar a regressão linear local da seguinte maneira:
```{r}
# kernel usado
# escrevendo 3 funcoes de kernel
# kernel gaussiano
gauss_kernel = function(x, x_i, h){
  return(1/sqrt(2*pi*(h^2))*exp(-(x - x_i)^2/(2*h^2)))
}

# epanechnikov
epanech_kernel = function(x, x_i, h){
  return(1 - ((x - x_i)^2/(h^2))*((x - x_i)^2 <= h))
}

# unifrome
unif_kernel = function(x, x_i, h){
  return((x - x_i)^2 <= h)
}


loc_lm = function(x_train, y_train, x_test, h, kernel){
  # calculando os pesos para cada x_test através do kernel e normalizando o vetor
  w = lapply(x_test, eval(kernel), x_i = x_train, h = h) %>% unlist() %>% matrix(
    nrow = length(x_train), ncol = length(x_test))
  w = sweep(w, 2, colSums(w), FUN = '/')
  
  # matriz de covariaveis com relacao a y, levando em conta o intercepto
  covs = model.matrix(y_train ~ x_train)
  estims = matrix(ncol = 2, nrow = length(x_test))
  # computando os estimadores de beta0 e beta1 localmente para cada x do teste
  for(i in 1:length(x_test)){
    omega = diag(w[,i])
    estims[i, ] = solve(t(covs) %*% omega %*% covs) %*% (t(covs) %*% omega %*% y_train)
  }
  # retornando as estimativas
  colnames(estims) = c("beta0", "beta1")
  return(estims)
}

fit_loc_lm = function(x, x_train, y_train, h, kernel){
  estims = loc_lm(x_train, y_train, x, h, kernel)
  vars = cbind(rep(1, length(x)),
               x)
  preds = estims * vars
  return(rowSums(preds))
}
```
Simulando alguns dados utilizando senos e cossenos para formar um comportamento de onda:
```{r}
# conjunto inteiro
n = 400
x_all = runif(n, -8, 8)
y_all = 2.5*cos(x_all) + 2.5*sin(x_all) + x_all + rnorm(n, sd = 1.25)

sim_data = data.frame(X = x_all, Y = y_all)
```
Formato do gráfico de dispersão:
```{r}
# regressao real
reg_real = function(x){
  return(2.5*cos(x) + 2.5*sin(x) + x)
}

sim_data %>%
  ggplot(aes(x = X, y = Y))+
  geom_point(color = "#0073C2FF", alpha = 0.75)+
  labs(x = "X",
       y = "Y",
       title = "Gráfico de dispersão dos dados simulados com regressão real",
       colour = "Regressão")+
  stat_function(fun = reg_real,
                aes(colour = "Regressao real"), size = 1.25)+
  theme_bw()+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
\
Separando o conjunto de treinamento e teste e testando inicialmente para apenas $h = 0.2$, mostrando as 10 primeiras observações de teste:
```{r}
set.seed(1250, sample.kind="Rounding")
n_train = 300
id_train = sample(1:n, size = n_train, replace = F)

x_train = x_all[id_train]
x_test = x_all[-id_train]
y_train = y_all[id_train]


coefs = loc_lm(x_train, y_train, x_test, h = 0.2, kernel = "gauss_kernel")
coefs %>% as.data.frame() %>% head(10)
```
Podemos analisar graficamente para diferentes valores de $h$ com o kernel gaussiano fixado, utilizando os dados de treinamento no range dado:
```{r}
# testando para diferentes h's
h = c(0.05, 0.2, 0.6, 1.25, 3.25)

p1 = sim_data %>%
  ggplot(aes(x = X, y = Y))+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[1], kernel = gauss_kernel),
                aes(colour = "h = 0.05"), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[2], kernel = gauss_kernel),
                aes(colour = paste0("h = ", h[2])), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[3], kernel = gauss_kernel),
                aes(colour = paste0("h = ", h[3])), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[4], kernel = gauss_kernel),
                aes(colour = paste0("h = ", h[4])), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[5], kernel = gauss_kernel),
                aes(colour = paste0("h = ", h[5])), size = 1)+
  labs(x = "X",
       y = "Y",
       title = "Gráfico de dispersão dos dados simulados com as regressões estimadas",
       colour = "h")+
  theme_bw()+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5)) +
  scale_colour_brewer(palette = "Set1")


p1
```
\
Se compararmos com a regressão verdadeira apenas usando $h = 0.2$ e $h = 0.6$, teremos:
```{r}
sim_data %>%
  ggplot(aes(x = X, y = Y))+
  labs(x = "X",
       y = "Y",
       title = "Gráfico de dispersão dos dados simulados com regressão real e local",
       colour = "Regressão")+
  stat_function(fun = reg_real,
                aes(colour = "Real"), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = 0.2, kernel = gauss_kernel),
                aes(colour = paste0("Local com h = 0.2")), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = 0.6, kernel = gauss_kernel),
                aes(colour = paste0("Local com h = 0.6")), size = 1)+
  theme_bw()+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))+
  scale_colour_brewer(palette = "Set1")
```
Notando-se que a regressão linear local com $h = 0.6$ tem uma aproximação relativamente boa da curva real, com uma melhor suavidade que a regressão local com $h = 0.2$. \

# Exercício 2: \
Lendo os dados e transformando os textos em matriz documento-texto:
```{r warning=FALSE, message=F}
library(tm)
library(wordcloud)
library(SnowballC)
av_texto = read.csv("/home/kuben/estatistica_UFSCAR/Mineracao de dados/listas/lista_2/TMDb_updated.CSV")
```
Checando se algum overview é vazio e removendo:
```{r}
av_texto %<>% filter(overview != "")
```
Fazendo um histograma para a variável resposta:
```{r}
av_texto %>%
  ggplot(aes(x = vote_average)) +
  geom_histogram(fill = "#0073C2FF", color = "black", bins = 20)+
  labs(x = "Nota média",
       y = "Frequência",
       title = "Histograma das notas médias")+
  theme(text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
Vemos que as notas vão de 0 a 10, tendo algumas com valor 0. Analisando também a distribuição da contagem de votos:
```{r}
av_texto %>%
  ggplot(aes(x = vote_count)) +
  geom_histogram(fill = "#0073C2FF", color = "black", bins = 20)+
  labs(x = "Contagem",
       y = "Frequência",
       title = "Histograma das contagens de voto")+
  theme(text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
Vemos que muitos filmes tiveram poucos votos, com alguns possivelmente não tendo votos e tendo nota 0 assimilada apenas por isso. Podemos filtrar tais filmes para evitar a presença de ruídos nos modelos que serão utilizados a seguir:
```{r}
av_texto %<>% filter(vote_count != 0)
```
**Item a**: \
Transformando a variável de avaliação "overview" em um corpus e depois removendo pontuações, números, espaço branco extra, stopwords e utilizando o IDF para penalização de palavras muito frequentes em cada resenha:
```{r}
# primeiro transformando em VCorpus
overview =  av_texto$overview %>% VectorSource %>% VCorpus(readerControl = 
                                                        list(language = "english"))
# removendo algumas palavras e stopwords
overview %<>% tm_map(removeWords, c("the", "and", stopwords("english")))

# Document Term Matrix
dtm = overview %>% DocumentTermMatrix(control = list(tolower = TRUE,
                                                     removePunctuation = TRUE,
                                                     removeNumbers = TRUE,
                                                     stripWhitespace = TRUE,
                                                     weighting = weightTfIdf,
                                                     stopwords = TRUE,
                                                     stemming = TRUE))

dtm
```
Checa-se uma alta esparcidade dos dados, com cerca de 22939 termos e 9970 documentos. Salienta-se que o máximo de termos de uma resenha nesse caso foi de 34. Podemos dar uma pequena olhada em algumas colunas e linhas da matriz documento termo da seguinte maneira:
```{r}
inspect(dtm[1:5, 500:505])
```
Checa-se a existência de uma grande esparsidade na matriz de documento inteira e no próprio exemplo tomado. Além disso, percebe-se a presença de muitos nomes na matriz anterior o que pode indicar que a matriz documento termo tem muitas palavras que se referem na verdade a nomes. Para a obtenção de uma matriz documento termo mais limpa, podemos remover os termos com maior esparsidade, estabelecendo um máximo de esparcidade para cada termo. Escolheremos um máximo de 0.99 de esparsidade:
```{r}
dtm %<>% removeSparseTerms(0.99)
dtm
```
Percebemos uma grande redução no número de termos, tendo agora 488 termos. Além disso a porcentagem de esparsidade deu uma pequena diminuída. Podemos checar novamente alguns termos nos primeiros 5 documentos novamente:
```{r}
inspect(dtm[1:5, 400:405])
```
Assim, antes de se seguir para a modelagem de fato, podemos averiguar a frequência dos termos atráves de um gráfico de Word Cloud:
```{r}
set.seed(1234, sample.kind="Rounding")
freq = data.frame(sort(colSums(as.matrix(dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words = 100, colors = brewer.pal(8, "Dark2"),
          random.order=FALSE, rot.per = 0.35, scale = c(2.15, .5))
```
Percebemos que as palavras "life", "new", "find" e "young" tem as maiores frequências entre as demais, tendo "man", "woman", "world", "friend" e entre outras como outras variáveis com maiores frequências. Podemos dividir agora o conjunto de dados em treinamento, validação e teste. Para que haja uma quantidade boa de observações para teste, podemos tomar $20\%$ do conjunto original que nos dão 1994 observações. Para o restante das $80\%$ das observações podemos dividir em $75\%$ de treinamento e $25\%$ de validação, ou seja, teremos 5982 das observações para treino e 1994 para validação, que é grande o suficiente para uma estimativa acurada do risco durante a realização de validação cruzada:
```{r warning = F}
# dividindo primeiro treino e teste
set.seed(750, sample.kind="Rounding")
n_train = 0.6 * nrow(av_texto)
n_valid = 0.2 * nrow(av_texto)
ids = 1:nrow(av_texto)
id_train = sample(ids, size = n_train, replace = F)
id_valid = sample(ids[-id_train], size = n_valid, replace = F)

# dividino as matrizes
x_train = dtm[c(id_train, id_valid), ]
x_train_valid = dtm[id_train, ]
x_valid = dtm[id_valid, ]
x_test = dtm[-c(id_train, id_valid), ]

y_train = av_texto$vote_average[c(id_train, id_valid)]
y_train_valid = av_texto$vote_average[id_train]
y_valid = av_texto$vote_average[id_valid]
y_test = av_texto$vote_average[-c(id_train, id_valid)]
```
**Item b**: \
Escolhendo $k$ por validação cruzada para o KNN, computando o risco para cada $k$ diferente:
```{r}
library(FNN)
tam = 50
k = 1:tam
risco = numeric(tam)

for(i in 1:tam){
  preds = knn.reg(train = x_train_valid, test = x_valid, y = y_train_valid, k = k[i])$pred
  risco[i] = (1/length(y_valid))*(sum(abs(preds - y_valid)))
}
```
Podemos avaliar o risco de acordo com cada $k$:
```{r}
data.frame(k = k,
           risco = risco) %>%
  ggplot(aes(x = k, y = risco)) +
  geom_point(color = "#0073C2FF", alpha = 0.75) +
  geom_line(color = "#0073C2FF",size = 1)+
  theme_bw()+
  labs(x = "k",
       y = "Erro médio absoluto",
       title = "Risco de acordo com a escolha de k")+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
\
Ou seja, verificamos que a partir de $k = 30$, o erro médio absoluto decai muito pouco, sendo praticamente constante. Podemos ver qual $k$ com menor risco:
```{r}
which.min(risco)
min(risco)
```
Podemos tomar então $k = 45$, tendo uma pequena diferença entre $k = 45$ e $k = 50$ como se pode ver abaixo:
```{r}
abs(risco[50] - risco[40])
```
Ou seja, não vale a pena aumentar mais o range de $k$, pois muito provavelmente a diferença entre os riscos não será tão considerável. Assim, tomaremos $k = 45$ como número de vizinhos. Fixando portanto o número de vizinhos, podemos obter o risco (tomado como erro absoluto médio) estimado e o intervalo de confiança como segue:
```{r}
preds_knn = knn.reg(train = x_train, test = x_test, y = y_train, k = 45)$pred

medidas_knn = data.frame(
  risco_knn = 1/length(y_test)*(sum(abs(preds_knn - y_test))))

std_error = function(loss_func, preds, y){
  SD = sqrt((1/length(y))*mean((abs(preds_knn - y) - (loss_func(preds_knn, y)))^2))
  return(2*SD) 
}

errors = c(std_error(MAE, preds, y_test))
# calculando erro padrao para cada metodo
medidas_knn$IC_lower = medidas_knn$risco_knn - errors
medidas_knn$IC_upper = medidas_knn$risco_knn + errors
medidas_knn
```
Ou seja, percebemos um pequeno erro médio absoluto, com um intervalo de confiança não tão grande. \
**Item c:** \
Agora ajustaremos um modelo de regressão com regularização lasso, usando o conjunto de treinamento inteiro e usando a validação já interna no algoritmo do glmnet para obter $\lambda$ por validação cruzada utilizando uma matriz esparsa do tipo dgMattrix pelo comando \textit{sparseMatrix} para representar a matriz documento termo:
```{r}
set.seed(1275)
# tomando os valores de coeficiente do lasso pela validacao cruzada:
cv.lasso = sparseMatrix(i = x_train$i, j = x_train$j, x = x_train$v,
                         dimnames = list(NULL, x_train$dimnames[[2]]),
                         dims = c(x_train$nrow, x_train$ncol)) %>%
  cv.glmnet(y_train, alpha = 1, stantardize = T)

# melhores valores de lambda
cv.lasso$lambda.min
cv.lasso$lambda.1se
```
Ou seja, o $\lambda$ mínimo é aproximadamente 0.0104, com o $\lambda$ com maior penalização é aproximadamente 0.2. Podemos checar também o Risco contra os valores de $\lambda$:
```{r}
plot(cv.lasso)
```
\
Percebemo que o $\lambda$ mínimo diminui o número de covariaveis de 488 para 245, tendo portanto uma regularização adequada, enquanto que o $\lambda$ com maior penalização possui 123 covariaveis selecionadas, tendo portanto uma filtragem suave das covariáveis. Podemos checar a importância das covariáveis como segue:
```{r}
# usando matriz esparsa
mod_lasso = sparseMatrix(i = x_train$i, j = x_train$j, x = x_train$v,
                         dimnames = list(NULL, x_train$dimnames[[2]]),
                         dims = c(x_train$nrow, x_train$ncol)) %>%
  glmnet(y_train, alpha = 1, 
                  lambda = cv.lasso$lambda.min, stantardize = T)

coefs_data = data.frame(coefs = coef(mod_lasso)[-1],
                        names = as.factor(row.names(coef(mod_lasso))[-1])) %>%
  filter(coefs != 0)
```
```{r}
sliced_coefs_data = coefs_data %>%
  filter(coefs != 0)%>%
  mutate(negative = as.factor(ifelse(coefs < 0, "negativo",
                                     "positivo"))) %>%
  arrange(coefs) %>%
  dplyr::slice(c(1:20, (nrow(coefs_data) - 19):nrow(coefs_data)))

sliced_coefs_data %>%
  mutate(names = fct_reorder(names, abs(coefs), .desc = F)) %>%
  ggplot(aes(x = names, y = coefs, fill = negative))+
  geom_bar(stat = "identity")+
  labs(title = "Coeficientes do modelo de regressão Lasso ajustado",
       y = "Valores dos coeficientes",
       fill = "Coeficiente")+
  coord_flip()+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))+
  facet_wrap(~negative, scales = "free", ncol = 2)+
  scale_fill_brewer(palette = "Set1")
```
Ou seja, nota-se que as principais palavras positivas são "despit", "friendship", "war", "tell", "director", enquanto que as principais palavras negativas são "ancient", "creatur" e "group", indicando que um filme que possívelmente tem um diretor famoso, trabalha com a amizade de personagens no enredo e possivelmente há alguma reviravolta ou superação como indica a palavra com stemming "despit" (de "despite", traduzido para "apesar de" no português), levando a melhores notas, enquanto que um filme que tem a presença de criaturas (creature) em seu enredo, um contexto ou coisas antigas ("ancient"), grupo de amigos no enredo (group) e um contexto sobrevivência (surviv), tende a ter avaliações ruins. Por fim, calculemos o risco e seu intervalo de confiança:
```{r}
# matriz de preditores
x_test_sparse = sparseMatrix(i = x_test$i, j = x_test$j, x = x_test$v,
                         dimnames = list(NULL, x_test$dimnames[[2]]),
                         dims = c(x_test$nrow, x_test$ncol))

pred.lasso = predict(mod_lasso,
s = cv.lasso$lambda.min,
newx = x_test_sparse)

# calculando as medidas de validacao
medidas_lasso = data.frame(
  risco_lasso = 1/length(y_test)*(sum(abs(pred.lasso - y_test)))
)

errors = std_error(MAE, pred.lasso, y_test)
# calculando erro padrao para cada metodo
medidas_lasso$IC_lower = medidas_lasso$risco_lasso - errors
medidas_lasso$IC_upper = medidas_lasso$risco_lasso + errors
medidas_lasso
```
Percebe-se uma certa reduzida no risco do lasso em comparação ao do KNN, com seu intervalo de confiança também reduzindo. \
**Item d:** \
Podemos ajustar um modelo de Florestas aleatórias usando como medida de importância a impureza de cada variável e selecionando $\left \lfloor {\sqrt{p}} \right \rfloor$ variáveis para cada árvore de decisão:
```{r}
library(ranger)
x_train_sparse = sparseMatrix(i = x_train$i, j = x_train$j, x = x_train$v,
                         dimnames = list(NULL, x_train$dimnames[[2]]),
                         dims = c(x_train$nrow, x_train$ncol))
mat = cbind(x_train_sparse,y_train)
rf_texto = ranger(data = mat, dependent.variable.name = "y_train", 
                  importance = "impurity", seed = 1776, verbose = F)
```
Assim, obtemos a estimativa de risco com o conjunto de teste:
```{r}
preds_rf = rf_texto %>%
  predict(data = x_test_sparse)
```

```{r}
# calculando as medidas de validacao
medidas_rf = data.frame(
  risco_random_forest = 1/length(y_test)*(sum(abs(preds_rf$predictions - y_test)))
)
 
errors = std_error(MAE, preds_rf$predictions, y_test)

# calculando erro padrao para cada metodo
medidas_rf$IC_lower = medidas_rf$risco_random_forest - errors
medidas_rf$IC_upper = medidas_rf$risco_random_forest + errors
medidas_rf
```
Notamos ainda uma redução no risco em comparação ao KNN, tendo porém a regularização lasso com menor risco ainda. Podemos analisar a importância das variáveis como segue:
```{r}
import = tibble(variable = names(ranger::importance(rf_texto)),
                importance = ranger::importance(rf_texto)) %>%
  arrange(desc(importance))
import %>% top_n(n = 25) %>%
  ggplot(aes(x = reorder(variable, importance),
            y = importance, fill = importance))+
  geom_bar(stat = "identity", position = "dodge") + coord_flip()+
  labs(y = "Importância de variável",
       x = "",
       title = "Sumário da importâncias de variáveis",
       fill = "Importância")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))+
  scale_fill_gradient(low = "firebrick2", high = "dodgerblue3")
```
Percebemos algumas semelhanças e diferenças entre a importância de variável captada pela floresta aleatória em comparação ao modelo de regressão com lasso, tendo os termos "war"e "group" como importantes tanto para o lasso quanto o random forest. Porém, os termos "stori", "life" e "find" que são tidos como relativamente importantes pela floresta aleatória não são tão importantes no modelo linear com lasso, e termos que são importantes para o lasso como "ancient" e "creatur", "despit" e "friendship", não são tão importantes para a floresta aleatória. Em suma, "war" e "group" são as principais palavras em comum em termos de importância para os dois modelos, enquanto muitas palavras que são importantes para a floresta aleatória nem sequer são mostradas no gráfico de coeficientes do lasso, como é o caso de "young", "find", "film", e vice e versa. \
**item (e):** \
Podemos em seguida utilizar uma validação cruzada (k-fold) para escolher o melhor número de iteração $B$ para o Xgboost, fixando um early stopping de $20$, um total de iterações $1000$, uma taxa de aprendizado $\lambda = 0.01$, e uma profundidade 6 de árvore (hiper-parâmetros default no xgboost):
```{r message = F}
set.seed(1500)
library(xgboost)
# ajustando xgboost na matriz de treinamento esparsa
xgb_cv = xgb.cv(data = x_train_sparse,
                label = y_train,
                nrounds = 1000,
                nfold = 5,
                eta = 0.01,
                early_stopping_rounds = 20,
                verbose = F)
```
Obtendo-se como melhor iteração:
```{r}
xgb_cv$best_iteration
```
Tendo a raiz quadrada do erro quadrático médio por iteração com seu erro padrão associado (estimado pelo k-fold):
```{r}
historico = xgb_cv$evaluation_log
historico %>%
  ggplot(aes(x = iter, y = test_rmse_mean)) +
  geom_line(color = "#0073C2FF",size = 1)+
  geom_ribbon(aes(y = test_rmse_mean, ymin = test_rmse_mean - 2*test_rmse_std,
                  ymax = test_rmse_mean + 2*test_rmse_std), color = "#0073C2FF",
              alpha = 0.35) +
  theme_bw()+
  labs(x = "Iterações",
       y = "Raiz do erro quadrático médio",
       title = "Risco para cada iteração no conjunto de validação")+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
Notamos que a partir da iteração 400, não houve muita redução do risco. De fato, no boosting, por natureza, há certa propensão de sobreajuste após um número grande de iterações, com o risco tendendo a estabilizar ou aumentar depois de certo momento, utilizando-se do early stopping para justamente encontrar a iteração a partir da qual o risco apenas cresce. Assim, nesse problema em particular, obtemos 699 como o número de iterações que minimiza o risco no conjunto de validação, tendo um crescimento continuo do risco após tal iteração. Assim, com $\lambda = 0.01$ e $B = 699$, mantendo ainda um early stopping de 20 iterações, obtemos o ajuste por xgboost com uma profundidade de árvore 6:
```{r}
xgb_texto = xgboost(data = x_train_sparse,
                    label = y_train,
                    nrounds = xgb_cv$best_iteration,
                    early_stopping_rounds = 20,
                    eta = 0.01,
                    verbose = F)
```
Obtendo-se o risco estimado no conjunto de teste e o intervalo de confiança associado:
```{r}
preds_xgb = xgb_texto %>%
  predict(x_test_sparse)
```
```{r}
# calculando as medidas de validacao
medidas_xgb = data.frame(
  risco_xgb = 1/length(y_test)*(sum(abs(preds_xgb - y_test)))
)
 
errors = std_error(MAE, preds_xgb, y_test)

# calculando erro padrao para cada metodo
medidas_xgb$IC_lower = medidas_xgb$risco_xgb - errors
medidas_xgb$IC_upper = medidas_xgb$risco_xgb + errors
medidas_xgb
```
Observa-se um risco um pouquinho maior que o da floresta aleatória, e um intervalo de confiança com limites também um pouco mais amplos. Assim, o lasso segue sendo o modelo com melhor perfomance dentre os demais. Por fim, podemos obter a importância das variaveis para as 25 mais importantes:
```{r}
mat_imp = xgb.importance(feature_names = colnames(x_train),model = xgb_texto)

mat_imp %>% top_n(n = 25) %>%
  ggplot(aes(x = reorder(Feature, Gain),
            y = Gain, fill = Gain))+
  geom_bar(stat = "identity", position = "dodge") + coord_flip()+
  labs(y = "Contribuição de cada variável",
       x = "",
       title = "Sumário da importâncias de variáveis para o Xgboost",
       fill = "Contribuição")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))+
  scale_fill_gradient(low = "firebrick2", high = "dodgerblue3")
```
Notamos grande similaridade da distribuição de importância para o Xgboost em comparação ao florestas aleatórias, com as 5 variáveis mais importantes sendo as mesmas: "stori, "life", "group", e "find", tal que "stori" é a variável com maior importância disparada, com a maior diferença se dando pela palavra "war" não ser tão importante para o boosting. Assim, mantém-se os mesmos comentários feitos na comparação entre a importância de variáveis associada a floresta aleatória e para o modelo de regressão lasso, ou seja, apesar de certas diferenças, as principais variáveis mais importantes são as mesmas. \
**item (f):** \
Inicialmente podemos ajustar uma rede neural feedforward com 3 camadas, com 64, 32 e 16 neurônios respectivamente, todas com função de ativação Relu, tomando um total de 1000 épocas com early stopping de 20 iterações, um tamanho de batch fixado de 32 (default do keras) e uma taxa de aprendizado de 0.005 no otimizador \textit{adam}:
```{r message = F}
library(keras)
# arquitetura da rede neural feed forward
nn_texto = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)

# funcao objetivo especificada
nn_texto %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.005),
  metrics = list("mean_absolute_error")
)

ajuste_nn = nn_texto %>% keras::fit(
  scale(as.matrix(x_train)),
  y_train,
  epochs = 1000,
  validation_split = 0.25,
  callbacks = list(callback_early_stopping(monitor = "loss", patience = 30)),
  verbose = F
)
```
Podemos averiguar como se dá o desempenho da rede neural no conjunto de validação para todas as épocas selecionadas:
```{r}
plot(ajuste_nn, metrics = "loss")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5),
        legend.position = "top")+
  labs(x = "Época",
       y = "Risco",
       fill = "Dados",
       colour = "Dados")+
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1")+
  ylim(0, 12)
```
Verifica-se uma diminuição rápida do risco no conjunto de treinamento e validação, havendo um early stopping antes da 200ª época. Caso chequemos o desempenho da rede neural no conjunto de teste, obtemos o risco e seu intervalo de confiança:
```{r}
# escalonando a matriz de teste com base nas medidas do treinamento
mu_train = x_train %>%
  as.matrix() %>%
  as.data.frame() %>%
  colMeans()

sd_train = x_train %>%
  as.matrix() %>%
  as.data.frame() %>%
 apply(2, sd)

preds_nn = nn_texto %>%
  predict(scale(as.matrix(x_test), center = mu_train, scale = sd_train))
```
```{r}
# calculando as medidas de validacao
medidas_nn = data.frame(
  risco_nn = 1/length(y_test)*(sum(abs(as.numeric(preds_nn) - y_test)))
)
 
errors = std_error(MAE, as.numeric(preds_nn), y_test)

# calculando erro padrao para cada metodo
medidas_nn$IC_lower = medidas_nn$risco_nn - errors
medidas_nn$IC_upper = medidas_nn$risco_nn + errors
medidas_nn
```
Verifica-se um elevado risco para essa rede neural juntamente com um intervalo de confiança amplo, tendo ainda o modelo lasso como o melhor. Podemos adicionar um dropout para cada camada com certa porcentagem cada, tendo um dropout de 50% na primeira, 40% na segunda e 30% na ultima camada:
```{r}
# repetindo mesmo procedimento mas adicionando dropout
# arquitetura da rede neural feed forward

nn_texto_drop = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 1)

# funcao objetivo especificada
nn_texto_drop %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.001),
  metrics = list("mean_absolute_error")
)

ajuste_nn_drop = nn_texto_drop %>% keras::fit(
  scale(as.matrix(x_train)),
  y_train,
  epochs = 1000,
  validation_split = 0.25,
  callbacks = list(callback_early_stopping(monitor = "loss", patience = 30)),
  verbose = F
)
```
Com o seguinte desempenho da rede neural com dropout por época no conjunto de treino e validação:
```{r}
plot(ajuste_nn_drop, metrics = "loss")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5),
        legend.position = "top")+
  labs(x = "Época",
       y = "Risco",
       fill = "Dados",
       colour = "Dados")+
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1")+
  ylim(0, 12)
```
Percebe-se uma considerável melhora no desempenho do conjunto de validação, com uma grande redução da curva de risco, apesar de o conjunto de treino ter uma piora no desempenho. Além disso, atingiu-se a 200ª época antes do early stopping com uma relativa estabilidade nos riscos. Espera-se assim, que o risco para o conjunto de teste seja relativamente menor em comparação a rede neural sem dropout: 
```{r}
preds_nn_drop = nn_texto_drop %>%
  predict(scale(as.matrix(x_test), center = mu_train, 
                scale = sd_train))
```
```{r}
# calculando as medidas de validacao
medidas_nn_drop = data.frame(
  risco_nn = 1/length(y_test)*(sum(abs(as.numeric(preds_nn_drop) - y_test)))
)
 
errors = std_error(MAE, as.numeric(preds_nn_drop), y_test)

# calculando erro padrao para cada metodo
medidas_nn_drop$IC_lower = medidas_nn_drop$risco_nn - errors
medidas_nn_drop$IC_upper = medidas_nn_drop$risco_nn + errors
medidas_nn_drop
```
Percebe-se uma grande diminuida do risco da rede neural ao se utilizar dropout para cada camada da rede, tendo porém um risco maior que a floresta aleatória. Assim, obtém-se grandes melhoras no desempenho da rede neural definida ao se utilizar dropout em cada camada. \
**item (g):** \
Por fim, podemos ajustar uma kernel ridge regression com um kernel gaussiano, fixando $\lambda = 0.5$ e algum valor bom para a largura de banda $h$. Para tal, podemos implementar o kernel ridge regression com kernel gaussiano e através de uma pequena validação cruzada em um subconjunto do conjunto de treinamento total determinar um valor bom para $h$, realizando dessa forma um pequeno tuning: 
```{r}
# implementando primeiro a matriz de gram
gram = function(x, h){
  return(exp(-(as.matrix(dist(x))^2)/(2*(h^2))))
}

# implementando o krr
KRR = function(x, lambda, h, y){
  K = gram(x, h)
  # decomposicao espectral
  ev = eigen(K)
  U = ev$vectors
  L = diag(ev$values)
  eq_inv = t(solve(U)) %*% solve(L + lambda*diag(1, nrow = nrow(x))) %*% solve(U)
  return(eq_inv %*% y)
}
  
  
pred_krr = function(alpha, x_test, x, h){
  k = matrix(nrow = nrow(x), ncol = nrow(x_test))
  for(i in 1:nrow(x_test)){
    dist_quad = rowSums(sweep(x, 2, x_test[i, ])^2)
    k[, i] = exp(-(dist_quad)/(2*h^2))
  }
  return(as.numeric(t(alpha) %*% k))
}
# implementando a validacao cruzada para h em certo range, de 1 a 20
cv_KRR = function(x_train, x_valid, y_train, y_valid, lambda, ngrid){
  h = 1:ngrid
  risco = numeric(length(h))
  for(i in 1:length(h)){
    alpha = KRR(x_train, lambda, h[i], y_train)
    preds = pred_krr(alpha, x_valid, x_train, h[i])
    risco[i] = (1/length(y_valid))*sum(abs(preds - y_valid))
  }
  return(risco)
}

# tomando apenas 1000 observações para treino e validação
sample_subconj = sample(1:nrow(x_train), 1000, replace = F)
subconj_train = x_train[sample_subconj[1:700], ]
y_subconj_train = y_train[sample_subconj[1:700]]
y_subconj_valid = y_train[sample_subconj[701:1000]]
subconj_valid = x_train[sample_subconj[701:1000], ]

riscos = cv_KRR(as.matrix(subconj_train), as.matrix(subconj_valid), 
       y_subconj_train, y_subconj_valid, lambda = 0.5, 20)
```
```{r}
riscos_data_krr = data.frame(it = 1:20,
                             riscos = riscos)
riscos_data_krr %>%
  ggplot(aes(x = it, y = riscos))+
  geom_point(color = "#0073C2FF", alpha = 0.75) +
  geom_line(color = "#0073C2FF",size = 1)+
  theme_bw()+
  labs(x = "k",
       y = "Erro médio absoluto",
       title = "Risco de acordo com a escolha de k")+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
Ou seja, podemos tomar $h = 4$ com $\lambda = 0.5$ e estimar $\alpha$ pelos dados de treinamento:
```{r}
alpha_krr = KRR(as.matrix(x_train), lambda = 0.5, h = 4, y_train)
```
Predizendo em seguida o conjunto de teste:
```{r}
preds_krr = pred_krr(alpha_krr, as.matrix(x_test), as.matrix(x_train), h = 4)
```
```{r}
# calculando as medidas de validacao
medidas_krr = data.frame(
  risco_krr = 1/length(y_test)*(sum(abs(as.numeric(preds_krr) - y_test)))
)
 
errors = std_error(MAE, as.numeric(preds_krr), y_test)

# calculando erro padrao para cada metodo
medidas_krr$IC_lower = medidas_krr$risco_krr - errors
medidas_krr$IC_upper = medidas_krr$risco_krr + errors
medidas_krr
```
Vemos um desempenho melhor que o modelo lasso, com um intervalo de confiança também menor, porém tudo a um custo computacional elevado. \
**item (h):** \
Por fim, podemos plotar os valores preditos versus os observados para cada modelo como a seguir:
```{r}
# juntando todas as predicoes
all_preds = data.frame(resp = rep(y_test, 6),
                       preds = c(preds_knn, as.numeric(pred.lasso), preds_rf$predictions,
                                preds_xgb, as.numeric(preds_nn_drop), preds_krr),
                       mods = c(rep("KNN", length(y_test)), rep("Lasso", length(y_test)),
                                rep("Floresta aleatórias", length(y_test)),
                                rep("Xgboost", length(y_test)),
                                rep("NN com dropout", length(y_test)),
                                rep("KRR", length(y_test))))

all_preds %>%
  ggplot(aes(x = preds, y = resp)) +
  geom_point(color = "#0073C2FF", alpha = 0.55)+
  geom_abline(intercept = 0, slope = 1, colour = "red", linetype = "dashed") +
  xlim(min(all_preds$preds), max(all_preds$preds)) +
  labs(y = "Nota do filme",
       x = "Predições")+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))+
  facet_wrap(~mods)
```
Vemos que cada modelo tem a predição geralmente entre as notas 5 e 7, com o lasso e KRR seguindo mais a linearidade  da reta identidade, enquanto o restante dos modelos se dispersam apenas em torno de tal reta, com predições entre 5 e 7, formando um tipo de glóbulo. Ou seja, os modelos parecem não predizer tão bem as notas dos filmes, podendo ser interessante adicionar outras covariáveis que deem informações adicionais sobre cada filme, como a contagem de votos e a linguagem do filme. \
**item (i):** \
Para analisar a performance de cada modelo conjuntamente, podemos analisar a estimativa do risco e erros padrões de forma gráfica:
```{r}
# concatenando riscos e erros padrões
all_medidas = mapply(c, medidas_knn, medidas_lasso, medidas_rf,
                    medidas_xgb, medidas_nn, medidas_nn_drop,
                    medidas_krr)

# transformando para data frame
all_medidas %<>% as.data.frame()
# nomes de cada modelo
all_medidas$names = as.factor(c("KNN", "Lasso", "RF", "Xgb", "NN", "NN_dropout", "KRR"))
colnames(all_medidas)[1] = "risco"

# plotando
all_medidas %>%
  mutate(names = fct_reorder(names, risco, .desc = F)) %>%
  ggplot(aes(x = names, y = risco))+
  geom_point() +
  geom_errorbar(aes(ymin = IC_lower, ymax = IC_upper)) +
  labs(y = "Risco",
       x = "Modelos",
       title = "Riscos estimados e intervalos de confiança para cada modelo")+
  theme_minimal()+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
Vemos pelo gráfico acima que em geral, os modelos de kernel ridge regression (KRR), lasso e floresta aleatória têm desempenho muito similar, apesar de existir certa diferença, com o KRR melhor que o lasso e o lasso melhor que a floresta aleatória. Vemos porém que para o Xgboost há certa piora de desempenho em comparação a esses outros 3, seguido pelo KNN, com as redes neurais (com e sem dropout) possuindo os piores desempenhos entre todos os modelos. Assim, por fim, podemos escolher como melhor modelo, o modelo lasso, por ter um desempenho quase igual ao KRR e não ser tão custoso quanto ou ter tantos tuning parameters, sendo ao mesmo tempo muito interpretavel em comparação tanto ao KRR quanto a floresta aleatória.

