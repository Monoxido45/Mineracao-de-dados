---
title: "Lista 2 - Mineração de dados"
author: "Luben, Luiz Piccin, Vinicius Hideki"
date: "10/4/2021"
header-includes:
  - \usepackage{amsmath}
  - \usepackage{xcolor}
output: 
  pdf_document:
    fig_caption: yes
    df_print: kable
latex_engine: texlive
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, out.width = '85%', fig.align = "center")
```
```{r message = FALSE, include = FALSE}
# importando os pacotes de interesse
library(ggplot2)
library(ggthemes)
library(ggpubr)
library(GGally)
library(ggcorrplot)
library(reshape2)
library(tidyverse)
library(magrittr)


# pacotes do lasso, ridge, elastic net, validação e tidyverse
library(glmnet)
library(caret)
library(tidyverse)
library(magrittr)

# usando o tidymodels
library(tidymodels)
```
# Exercício 1: \
Podemos implementar a regressão linear local da seguinte maneira:
```{r}
# kernel usado
# escrevendo 3 funcoes de kernel
# kernel gaussiano
gauss_kernel = function(x, x_i, h){
  return(1/sqrt(2*pi*(h^2))*exp(-(x - x_i)^2/(2*h^2)))
}

# epanechnikov
epanech_kernel = function(x, x_i, h){
  return(1 - ((x - x_i)^2/(h^2))*((x - x_i)^2 <= h))
}

# unifrome
unif_kernel = function(x, x_i, h){
  return((x - x_i)^2 <= h)
}


loc_lm = function(x_train, y_train, x_test, h, kernel){
  # calculando os pesos para cada x_test através do kernel e normalizando o vetor
  w = lapply(x_test, eval(kernel), x_i = x_train, h = h) %>% unlist() %>% matrix(
    nrow = length(x_train), ncol = length(x_test))
  w = sweep(w, 2, colSums(w), FUN = '/')
  
  # matriz de covariaveis com relacao a y, levando em conta o intercepto
  covs = model.matrix(y_train ~ x_train)
  estims = matrix(ncol = 2, nrow = length(x_test))
  # computando os estimadores de beta0 e beta1 localmente para cada x do teste
  for(i in 1:length(x_test)){
    omega = diag(w[,i])
    estims[i, ] = solve(t(covs) %*% omega %*% covs) %*% (t(covs) %*% omega %*% y_train)
  }
  # retornando as estimativas
  colnames(estims) = c("beta0", "beta1")
  return(estims)
}

fit_loc_lm = function(x, x_train, y_train, h, kernel){
  estims = loc_lm(x_train, y_train, x, h, kernel)
  vars = cbind(rep(1, length(x)),
               x)
  preds = estims * vars
  return(rowSums(preds))
}
```
Simulando alguns dados utilizando senos e cossenos para formar um comportamento de onda:
```{r}
# conjunto inteiro
n = 400
x_all = runif(n, -8, 8)
y_all = 2.5*cos(x_all) + 2.5*sin(x_all) + x_all + rnorm(n, sd = 1.25)

sim_data = data.frame(X = x_all, Y = y_all)
```
Formato do gráfico de dispersão:
```{r}
# regressao real
reg_real = function(x){
  return(2.5*cos(x) + 2.5*sin(x) + x)
}

sim_data %>%
  ggplot(aes(x = X, y = Y))+
  geom_point(color = "#0073C2FF", alpha = 0.75)+
  labs(x = "X",
       y = "Y",
       title = "Gráfico de dispersão dos dados simulados com regressão real",
       colour = "Regressão")+
  stat_function(fun = reg_real,
                aes(colour = "Regressao real"), size = 1.25)+
  theme_bw()+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
\
Separando o conjunto de treinamento e teste e testando inicialmente para apenas $h = 0.2$, mostrando as 10 primeiras observações de teste:
```{r}
set.seed(1250, sample.kind="Rounding")
n_train = 300
id_train = sample(1:n, size = n_train, replace = F)

x_train = x_all[id_train]
x_test = x_all[-id_train]
y_train = y_all[id_train]


coefs = loc_lm(x_train, y_train, x_test, h = 0.2, kernel = "gauss_kernel")
coefs %>% as.data.frame() %>% head(10)
```
Podemos analisar graficamente para diferentes valores de $h$ com o kernel gaussiano fixado, utilizando os dados de treinamento no range dado:
```{r}
# testando para diferentes h's
h = c(0.05, 0.2, 0.6, 1.25, 3.25)

p1 = sim_data %>%
  ggplot(aes(x = X, y = Y))+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[1], kernel = gauss_kernel),
                aes(colour = "h = 0.05"), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[2], kernel = gauss_kernel),
                aes(colour = paste0("h = ", h[2])), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[3], kernel = gauss_kernel),
                aes(colour = paste0("h = ", h[3])), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[4], kernel = gauss_kernel),
                aes(colour = paste0("h = ", h[4])), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = h[5], kernel = gauss_kernel),
                aes(colour = paste0("h = ", h[5])), size = 1)+
  labs(x = "X",
       y = "Y",
       title = "Gráfico de dispersão dos dados simulados com as regressões estimadas",
       colour = "h")+
  theme_bw()+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5)) +
  scale_colour_brewer(palette = "Set1")


p1
```
\
Se compararmos com a regressão verdadeira apenas usando $h = 0.2$ e $h = 0.6$, teremos:
```{r}
sim_data %>%
  ggplot(aes(x = X, y = Y))+
  labs(x = "X",
       y = "Y",
       title = "Gráfico de dispersão dos dados simulados com regressão real e local",
       colour = "Regressão")+
  stat_function(fun = reg_real,
                aes(colour = "Real"), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = 0.2, kernel = gauss_kernel),
                aes(colour = paste0("Local com h = 0.2")), size = 1)+
  stat_function(fun = fit_loc_lm, args = list(x_train = x_train, 
                                              y_train = y_train, 
                                              h = 0.6, kernel = gauss_kernel),
                aes(colour = paste0("Local com h = 0.6")), size = 1)+
  theme_bw()+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))+
  scale_colour_brewer(palette = "Set1")
```
# Exercício 2: \
Lendo os dados e transformando os textos em matriz documento-texto:
```{r}
library(tm)
library(wordcloud)
library(SnowballC)
av_texto = read.csv("/home/kuben/estatistica_UFSCAR/Mineracao de dados/listas/lista_2/TMDb_updated.CSV")
```
Checando se algum overview é vazio e removendo:
```{r}
av_texto %<>% filter(overview != "")
```
Fazendo um histograma para a variável resposta:
```{r}
av_texto %>%
  ggplot(aes(x = vote_average)) +
  geom_histogram(fill = "#0073C2FF", color = "black", bins = 20)+
  labs(x = "Nota média",
       y = "Frequência",
       title = "Histograma das notas médias")+
  theme(text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
**Item a**: \
Transformando a variável de avaliação "overview" em um corpus e depois removendo pontuações, numeros, espaço branco extra, stopwords e utilizando o IDF para penalização de palavras muito frequentes em cada resenha:
```{r}
# primeiro transformando em VCorpus
overview =  av_texto$overview %>% VectorSource %>% VCorpus(readerControl = 
                                                        list(language = "english"))
# removendo algumas palavras e stopwords
overview %<>% tm_map(removeWords, c("the", "and", stopwords("english")))

# Document Term Matrix
dtm = overview %>% DocumentTermMatrix(control = list(tolower = TRUE,
                                                     removePunctuation = TRUE,
                                                     removeNumbers = TRUE,
                                                     stripWhitespace = TRUE,
                                                     weighting = weightTfIdf,
                                                     stopwords = TRUE,
                                                     stemming = TRUE))

dtm
```
Checa-se uma alta esparcidade dos dados, com cerca de 22939 termos e 9970 documentos. Salienta-se que o máximo de termos de uma resenha nesse caso foi de 34. Podemos dar uma pequena olhada em algumas colunas e linhas da matriz documento termo da seguinte maneira:
```{r}
inspect(dtm[1:5, 500:505])
```
Checa-se a existência de uma grande esparsidade na matriz de documento inteira e no próprio exemplo tomado. Além disso, percebe-se a presença de muitos nomes na matriz anterior o que pode indicar que a matriz documento termo tem muitas palavras que se referem na verdade a nomes. Para a obtenção de uma matriz documento termo mais limpa, podemos remover os termos com maior esparsidade, estabelecendo um máximo de esparcidade para cada termo. Escolheremos um máximo de 0.99 de esparsidade:
```{r}
dtm %<>% removeSparseTerms(0.99)
dtm
```
Percebemos uma grande redução no número de termos, tendo agora 488 termos. Além disso a porcentagem de esparsidade deu uma pequena diminuída. Podemos checar novamente alguns termos nos primeiros 5 documentos novamente:
```{r}
inspect(dtm[1:5, 400:405])
```
Assim, antes de se seguir para a modelagem de fato, podemos averiguar a frequencia dos termos atráves de um gráfico de Word Cloud:
```{r}
set.seed(1234, sample.kind="Rounding")
freq = data.frame(sort(colSums(as.matrix(dtm)), decreasing=TRUE))
wordcloud(rownames(freq), freq[,1], max.words = 100, colors = brewer.pal(8, "Dark2"),
          random.order=FALSE, rot.per = 0.35, scale = c(2.15, .5))
```
Percebemos que as palavras "life", "new", "find" e "young" tem as maiores frequências entre as demais, tendo "man", "woman", "world", "friend" e entre outras como outras variáveis com maiores frequências. Podemos dividir agora o conjunto de dados em treinamento, validação e teste. Para que haja uma quantidade boa de observações para teste, podemos tomar $20\%$ do conjunto original que nos dão 1994 observações. Para o restante das $80\%$ das observações podemos dividir em $75\%$ de treinamento e $25\%$ de validação, ou seja, teremos 5982 das observações para treino e 1994 para validação, que é grande o suficiente para uma estimativa acurada do risco durante a realização de validação cruzada:
```{r}
# dividindo primeiro treino e teste
set.seed(750, sample.kind="Rounding")
n_train = 0.6 * nrow(av_texto)
n_valid = 0.2 * nrow(av_texto)
ids = 1:nrow(av_texto)
id_train = sample(ids, size = n_train, replace = F)
id_valid = sample(ids[-id_train], size = n_valid, replace = F)

# dividino as matrizes
x_train = dtm[id_train, ]
x_valid = dtm[id_valid, ]
x_test = dtm[-c(id_train, id_valid), ]

y_train = av_texto$vote_average[id_train]
y_valid = av_texto$vote_average[id_valid]
y_test = av_texto$vote_average[-c(id_train, id_valid)]
```
**Item b**: \
Escolhendo $k$ por validação cruzada para o KNN, computando o risco para cada $k$ diferente:
```{r}
library(FNN)
tam = 50
k = 1:tam
risco = numeric(tam)

for(i in 1:tam){
  preds = knn.reg(train = x_train, test = x_valid, y = y_train, k = k[i])$pred
  risco[i] = (1/length(y_valid))*(sum(abs(preds - y_valid)))
}
```
Podemos avaliar o risco de acordo com cada $k$:
```{r}
data.frame(k = k,
           risco = risco) %>%
  ggplot(aes(x = k, y = risco)) +
  geom_point(color = "#0073C2FF", alpha = 0.75) +
  geom_line(color = "#0073C2FF",size = 1)+
  theme_bw()+
  labs(x = "k",
       y = "Erro médio absoluto",
       title = "Risco de acordo com a escolha de k")+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
\
Ou seja, verificamos que a partir de $k = 30$, o erro médio absoluto decai muito pouco, sendo praticamente constante. O menor $k$, será $k = 50$, tendo:
```{r}
which.min(risco)
min(risco)
```
Podemos tomar $k = 50$, tendo uma pequena diferença entre $k = 40$ e $k = 50$ como se poder ver abaixo:
```{r}
abs(risco[50] - risco[40])
```
Ou seja, não vale a pena aumentar mais o range de $k$, pois muito provavelmente a diferença entre os riscos não será tão considerável. Assim, tomaremos $k = 50$ como número de vizinhos. Fixando portanto o número de vizinhos, podemos obter o risco estimado e o intervalo de confiança como segue:
```{r}
x_train = dtm[c(id_train, id_valid), ]
y_train = av_texto$vote_average[c(id_train, id_valid)]
preds = knn.reg(train = x_train, test = x_test, y = y_train, k = 50)$pred

medidas = data.frame(
  risco_knn = 1/length(y_test)*(sum(abs(preds - y_test))))

std_error = function(loss_func, preds, y){
  SD = sqrt((1/length(y))*mean((abs(preds - y) - (loss_func(preds, y)))^2))
  return(2*SD) 
}

errors = c(std_error(MAE, preds, y_test))
# calculando erro padrao para cada metodo
medidas$IC_lower = medidas$risco_knn - errors
medidas$IC_upper = medidas$risco_knn + errors
medidas
```
Ou seja, percebemos um pequeno erro médio absoluto, com um intervalo de confiança nao tão grande. \
**Item c:** \
Agora ajustaremos um modelo de regressão com regularização lasso, usando o conjunto de treinamento inteiro e usando a validação já interna no algoritmo do glmnet para obter $\lambda$ por validação cruzada:
```{r}
set.seed(1275)
# tomando os valores de coeficiente do lasso pela validacao cruzada:
cv.lasso = sparseMatrix(i = x_train$i, j = x_train$j, x = x_train$v,
                         dimnames = list(NULL, x_train$dimnames[[2]]),
                         dims = c(x_train$nrow, x_train$ncol)) %>%
  cv.glmnet(y_train, alpha = 1, stantardize = T)

# melhores valores de lambda
cv.lasso$lambda.min
cv.lasso$lambda.1se
```
Ou seja, o $\lambda$ mínimo é dado por aproximadamente 0.02, com o $\lambda$ com maior penalização sendo 0.187. Podemos checar também o Risco contra os valores de $\lambda$:
```{r}
plot(cv.lasso)
```
\
Percebemo que o $\lambda$ mínimo diminui o número de covariaveis de 488 para 138, tendo portanto uma regularização adequada, enquanto que o $\lambda$ com maior penalização possui apenas 1 covariavel selecionada, tendo portanto uma filtragem muito brusca das covariaveis. Podemos checar a importância das covariaveis como segue:
```{r}
# usando matriz esparsa
mod_lasso = sparseMatrix(i = x_train$i, j = x_train$j, x = x_train$v,
                         dimnames = list(NULL, x_train$dimnames[[2]]),
                         dims = c(x_train$nrow, x_train$ncol)) %>%
  glmnet(y_train, alpha = 1, 
                  lambda = cv.lasso$lambda.min, stantardize = T)

coefs_data = data.frame(coefs = coef(mod_lasso)[-1],
                        names = as.factor(row.names(coef(mod_lasso))[-1])) %>%
  filter(coefs != 0)
```
```{r}
sliced_coefs_data = coefs_data %>%
  filter(coefs != 0)%>%
  mutate(negative = as.factor(ifelse(coefs < 0, "negativo",
                                     "positivo"))) %>%
  arrange(coefs) %>%
  slice(c(1:20, (nrow(coefs_data) - 19):nrow(coefs_data)))

sliced_coefs_data %>%
  mutate(names = fct_reorder(names, abs(coefs), .desc = F)) %>%
  ggplot(aes(x = names, y = coefs, fill = negative))+
  geom_bar(stat = "identity")+
  labs(title = "Coeficientes do modelo de regressão Lasso ajustado",
       y = "Valores dos coeficientes",
       fill = "Coeficiente")+
  coord_flip()+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))+
  facet_wrap(~negative, scales = "free", ncol = 2)+
  scale_fill_brewer(palette = "Set1")
```
Ou seja, nota-se que as principais palavras positivas são "director", "friendship", "despit", "age", "villag", enquanto que as principais palavras negativas são "plot", "movi" e "univers", indicando que um filme que possívelmente tem um diretor famoso, trabalha com a amizade de personagens no enredo e possivelmente tem alguma reviravolta ou superação como indica a palavra com stemming "despit" (de "despite", traduzido para apesar de no portugues), tem tendência a ter melhores notas, enquanto que um filme que que tem a presença grandes planos (plot) no enredo, auto-citação no resumo (autocitação com a palavra movie) e também tem as palavras derivadas de universo nem seu resumo (que talvez inclua "salvar o universo" ou um plano que envovle o universo), tende a ter avaliações ruins. Por fim, calculemos o risco e seu intervalo de confiança:
```{r}
# matriz de preditores
x_test_sparse = sparseMatrix(i = x_test$i, j = x_test$j, x = x_test$v,
                         dimnames = list(NULL, x_test$dimnames[[2]]),
                         dims = c(x_test$nrow, x_test$ncol))

pred.lasso = predict(mod_lasso,
s = cv.lasso$lambda.min,
newx = x_test_sparse)

# calculando as medidas de validacao
medidas = data.frame(
  risco_lasso = 1/length(y_test)*(sum(abs(pred.lasso - y_test)))
)

errors = std_error(MAE, pred.lasso, y_test)
# calculando erro padrao para cada metodo
medidas$IC_lower = medidas$risco_lasso - errors
medidas$IC_upper = medidas$risco_lasso + errors
medidas
```
Percebe-se uma certa reduzida no risco do lasso em comparação ao do KNN, com seu intervalo de confiança também reduzido. \
**Item d:** \
Podemos ajustar um modelo de Florestas aleatórias usando como medida de importância a impureza de cada variável e selecionando $\left \lfloor {\sqrt{p}} \right \rfloor$ variáveis para cada árvore de decisão:
```{r}
library(ranger)
library(randomForest)
x_train_sparse = sparseMatrix(i = x_train$i, j = x_train$j, x = x_train$v,
                         dimnames = list(NULL, x_train$dimnames[[2]]),
                         dims = c(x_train$nrow, x_train$ncol))
mat = cbind(x_train_sparse,y_train)
rf_texto = ranger(data = mat, dependent.variable.name = "y_train", 
                  importance = "impurity", seed = 1776)
```
Assim, obtemos a estimativa de risco com o conjunto de teste:
```{r}
preds_rf = rf_texto %>%
  predict(data = x_test_sparse)
```

```{r}
# calculando as medidas de validacao
medidas = data.frame(
  risco_random_forest = 1/length(y_test)*(sum(abs(preds_rf$predictions - y_test)))
)
 
errors = std_error(MAE, preds_rf$predictions, y_test)

# calculando erro padrao para cada metodo
medidas$IC_lower = medidas$risco_random_forest - errors
medidas$IC_upper = medidas$risco_random_forest + errors
medidas
```
Notamos ainda uma redução no risco em comparação ao modelo linear com regularização lasso. Podemos analisar a importancias das variáveis como segue:
 
```{r}
import = tibble(variable = names(ranger::importance(rf_texto)),
                importance = ranger::importance(rf_texto)) %>%
  arrange(desc(importance))
import %>% top_n(n = 25) %>%
  ggplot(aes(x = reorder(variable, importance),
            y = importance, fill = importance))+
  geom_bar(stat = "identity", position = "dodge") + coord_flip()+
  labs(y = "Importância de variável",
       x = "",
       title = "Sumário da importâncias de variáveis",
       fill = "Importância")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))+
  scale_fill_gradient(low = "firebrick2", high = "dodgerblue3")
```
Percebemos algumas semelhanças e diferenças entre a importância de variável captada pela floresta aleatória em comparação ao modelo de regressão com lasso, tendo os termos "plot", "film", "world" e "movi" como importantes tanto para o lasso quanto o random forest. Porém, os termos "despit", "friendship" e "director" que são tidos como relativamente importantes pelo lasso não são tão importantes no modelo de arvores aleatórias, e termos que são importantes pela árvore aleatória como "stori" e "seri" não tão importantes. Mas, em suma, as principais variáveis importantes são condizentes. 
**item (e):** \
Podemos em seguida utilizar uma validação cruzada para escolher o melhor número de iteração $B$ para o Xgboost, fixando um early stopping de $20$, uma taxa de aprendizado $\lambda = 0.01$, e uma profundidade 6 de árvore:
```{r}
library(xgboost)
# ajustando xgboost na matriz de treinamento esparsa
xgb_cv = xgb.cv(data = x_train_sparse,
                label = y_train,
                nrounds = 1000,
                nfold = 5,
                eta = 0.01,
                early_stopping_rounds = 20,
                verbose = F)
```
Obtendo-se como melhor iteração:
```{r}
xgb_cv$best_iteration
```
Tendo a raiz quadrada do erro quadrático médio por iteração com seu erro padrão associado:
```{r}
historico = xgb_cv$evaluation_log
historico %>%
  ggplot(aes(x = iter, y = test_rmse_mean)) +
  geom_line(color = "#0073C2FF",size = 1)+
  geom_ribbon(aes(y = test_rmse_mean, ymin = test_rmse_mean - 2*test_rmse_std,
                  ymax = test_rmse_mean + 2*test_rmse_std), color = "#0073C2FF",
              alpha = 0.35) +
  theme_bw()+
  labs(x = "Iterações",
       y = "Raiz do erro quadrático médio",
       title = "Risco para cada iteração no conjunto de validação")+
  theme(text = element_text(size = 11, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))
```
Notamos que de fato, a partir da iteração 400, não houve muita redução do risco. Assim 502 iterações é mais que o suficiente para obter um bom ajuste. Assim, com $\lambda = 0.01$ e $B = 502$, mantendo ainda um early stopping de 20 iterações, obtemos o ajuste por xgboost com uma profundidade de árvore 6:
```{r}
xgb_texto = xgboost(data = x_train_sparse,
                    label = y_train,
                    nrounds = xgb_cv$best_iteration,
                    early_stopping_rounds = 20,
                    eta = 0.01,
                    verbose = F)
```
Obtendo-se o risco estimado no conjunto de teste e o intervalo de confiança associado:
```{r}
preds_xgb = xgb_texto %>%
  predict(x_test_sparse)
```
```{r}
# calculando as medidas de validacao
medidas = data.frame(
  risco_xgb = 1/length(y_test)*(sum(abs(preds_xgb - y_test)))
)
 
errors = std_error(MAE, preds_xgb, y_test)

# calculando erro padrao para cada metodo
medidas$IC_lower = medidas$risco_xgb - errors
medidas$IC_upper = medidas$risco_xgb + errors
medidas
```
Observa-se um risco maior que o da floresta aleatória, e um intervalo de confiança com limites mais amplos. Assim, a floresta aleatória segue sendo o modelo com melhor perfomance dentre os demais. Por fim, podemos obter a importância de variaveis para as 25 variáveis mais importantes:
```{r}
mat_imp = xgb.importance(feature_names = colnames(x_train),model = xgb_texto)

mat_imp %>% top_n(n = 25) %>%
  ggplot(aes(x = reorder(Feature, Gain),
            y = Gain, fill = Gain))+
  geom_bar(stat = "identity", position = "dodge") + coord_flip()+
  labs(y = "Contribuição de cada variável",
       x = "",
       title = "Sumário da importâncias de variáveis para o Xgboost",
       fill = "Contribuição")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5))+
  scale_fill_gradient(low = "firebrick2", high = "dodgerblue3")
```
Notamos grande similaridade da distribuição de importância para o Xgboost em comparação ao florestas aleatórias, com as 5 variáveis mais importantes sendo as mesmas: "plot", "film", "movi", "world" e "final", tal que "plot" é a variável com maior importância disparada. Assim, mantém-se os mesmos comentários feitos na comparação entre a importância de variáveis associada a floresta aleatória e para o modelo de regressão lasso, ou seja, apesar de certas diferenças, as principais variáveis mais importantes são as mesmas. \
**item (f):** \
Inicialmente podemos ajustar uma rede neural feedforward com 3 camadas,com 64, 32 e 16 neurônios respectivamente, todas com função de ativação Relu, tomando um total de 1000 épocas com early stopping de 20 iterações, um tamanho de batch fixado de 32 (default do keras) e uma taxa de aprendizado de 0.005 no otimizador \textit{adam}:
```{r}
library(keras)
# arquitetura da rede neural feed forward
nn_texto = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dense(units = 1)

# funcao objetivo especificada
nn_texto %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.005),
  metrics = list("mean_absolute_error")
)

ajuste_nn = nn_texto %>% keras::fit(
  scale(as.matrix(x_train)),
  y_train,
  epochs = 1000,
  validation_split = 0.25,
  callbacks = list(callback_early_stopping(monitor = "loss", patience = 30)),
  verbose = F
)
```
Podemos averiguar como se dá o desempenho da rede neural no conjunto de validação para todas as épocas selecionadas:
```{r}
plot(ajuste_nn, metrics = "loss")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5),
        legend.position = "top")+
  labs(x = "Época",
       y = "Risco",
       fill = "Dados",
       colour = "Dados")+
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1")+
  ylim(0, 12)
```
Verifica-se uma diminuição rápida do risco no conjunto de treinamento e uma diminuição gradativa no conjunto de teste, havendo um early stopping antes da 200ª época. Caso chequemos o desempenho da rede neural no conjunto de teste, obtemos o risco e seu intervalo de confiança:
```{r}
preds_nn = nn_texto %>%
  predict(scale(as.matrix(x_test)))
```
```{r}
# calculando as medidas de validacao
medidas = data.frame(
  risco_nn = 1/length(y_test)*(sum(abs(as.numeric(preds_nn) - y_test)))
)
 
errors = std_error(MAE, as.numeric(preds_nn), y_test)

# calculando erro padrao para cada metodo
medidas$IC_lower = medidas$risco_nn - errors
medidas$IC_upper = medidas$risco_nn + errors
medidas
```
Verifica-se um elevado risco para essa rede neural juntamente com um intervalo de confiança amplo, tendo ainda o modelo de floresta aleatória como o melhor. Podemos adicionar um dropout para cada camada com certa porcentagem cada, tendo um dropout de 50% na primeira, 40% na segunda e 30% na ultima camada:
```{r}
# repetindo mesmo procedimento mas adicionando dropout
# arquitetura da rede neural feed forward

nn_texto_drop = keras_model_sequential() %>%
  layer_dense(units = 64, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 1)

# funcao objetivo especificada
nn_texto_drop %>% compile(
  loss = "mse",
  optimizer = optimizer_adam(learning_rate = 0.005),
  metrics = list("mean_absolute_error")
)

ajuste_nn_drop = nn_texto_drop %>% keras::fit(
  scale(as.matrix(x_train)),
  y_train,
  epochs = 1000,
  validation_split = 0.25,
  callbacks = list(callback_early_stopping(monitor = "loss", patience = 30)),
  verbose = F
)
```
Com o seguinte desempenho da rede neural com dropout por época no conjunto de treino e validação:
```{r}
plot(ajuste_nn_drop, metrics = "loss")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        text = element_text(size = 12, 
                            family ="serif"),
        plot.title = element_text(hjust = 0.5),
        legend.position = "top")+
  labs(x = "Época",
       y = "Risco",
       fill = "Dados",
       colour = "Dados")+
  scale_fill_brewer(palette = "Set1") +
  scale_colour_brewer(palette = "Set1")+
  ylim(0, 12)
```
Percebe-se uma considerável melhora no desempenho do conjunto de validação, com uma grande redução da curva de risco, apesar de o conjunto de treino ter uma piora no desempenho. Além disso, atingiu-se a 200ª época antes do early stopping com uma relativa estabilidade nos riscos. Espera-se assim, que o risco para o conjunto de teste seja relativamente menor em comparação a rede neural sem dropout: 
```{r}
preds_nn = nn_texto_drop %>%
  predict(scale(as.matrix(x_test)))
```
```{r}
# calculando as medidas de validacao
medidas = data.frame(
  risco_nn = 1/length(y_test)*(sum(abs(as.numeric(preds_nn) - y_test)))
)
 
errors = std_error(MAE, as.numeric(preds_nn), y_test)

# calculando erro padrao para cada metodo
medidas$IC_lower = medidas$risco_nn - errors
medidas$IC_upper = medidas$risco_nn + errors
medidas
```
Percebe-se uma grande diminuida do risco da rede neural ao se utilizar dropout para cada camada da rede, obtendo-se inclusive um risco menor que a floresta aleatória na terceira casa decimal. Assim, obtém-se grandes melhoras no desempenho da rede neural definida ao se utilizar dropout em cada camada, sendo agora este o melhor modelo dentre os demais utilizados. \
**item (g)**




